[описание задачи](https://scitator.notion.site/e5db49d792f6476a8b3ce19fd91c6655)

## Отчет

### Level 1.

Одной из первых на данном этапе была задача адаптации модели для генерации более позитивных текстов (поскольку исходная версия модели учится и на позитивных, и на негативных отзывах, соответственно, и генерирует тоже оба класса отзывов). И один из вариантов решения -- вставка ключевых слов в промпт, что и было реализовано. В целом при решении были следующие идеи:
- **Beam Search**: во время beam search можно оставлять такие опции, которые не только более вероятны, но и имеют положительную тональность. Это можно сделать, например, при помощи той же gpt2, если взять другую архитектуру, но тогда нужно будет переделывать функцию генерации, что, во-первых, не очень очевидно, во-вторых, будет не совсем эффективно. 
- **The more the better**: генерация как можно большего числа примеров. Однако этот вариант не является модификацией генерации. 
- **(Supervised) fine-tuning**: для реализации этой идеи нужно брать позитивные отзывы, например, из IMDB датасета, однако вновь такой подход не будет адаптацией модели. 
- **Prompt Engineering**: выбран с этой целью 'Good Review', он даёт результаты, как показано в соответствующем ноутбуке. Также была попытка вставить другие ключевые слова через bias, чтобы побуждать модель генерировать нужное.

В продолжение эксперимента из этой части считался reward с помощью "lvwerra/distilbert-imdb" для каждого из сгенерированных текстов. Все те, у которых reward был больше 2.5, считались положительными (больший threshold приводил, например, к тому, что в rejected оказывались недостаточно подробные положительные отзывы). Негативными текстами считались те, у которых reward был ниже 1.75.  Если оставлять упомянутый ранее bias, то часть текстов будут сильно похожи друг на друга, поэтому bias лучше убрать. Эту же проблему помогает решить и добавление фильтра по diversity, который, однако, работает весьма долго. 
При обучении sft модели с лоссом из статьи на созданном из пар winner-loser датасете получаются следующие результаты:

$$
\begin{array}{lllc}
\hline \boldsymbol{experiment } & \boldsymbol{reward} & \boldsymbol{diversity } \\
\hline \text {default} & \text{-0.068486} & \text{5.825884} \\
\text {hinge} & \text{ 2.533579} & \text{4.582645} \\
\text {sigmoid} & \text{ 2.538272} & \text{4.583957} \\
\text {best} & \text{ 2.935000} & \text{4.732000} \\
\hline
\end{array}
$$

Первое, что стоит отметить: reward генерируемых текстов действительно увеличился по сравнению с исходной sft политикой. Тем не менее, diversity наоборот снизилось, что, в целом, ожидаемо, поскольку мы стараемся генерировать именно позитивные отзывы. Если же сравнивать лоссы (hinge и sigmoid), то видно, что их значения очень близки и статистической разницы нет. 
Внимания заслуживает "best" -- это распределение из winners, получившееся после отбора кандидатов, reward стал выше, как и diversity.


### Level 2.

Здесь, в первую очередь, стоит заметить, что лосс в обобщённом виде, где f′ -- это производная произвольной дивергенции, в DPOTrainer является обратной КЛ-дивергенцией, которую и нужно менять. Опорой для кода в этом задании служит таблица с уже посчитанными производными:

$$
\begin{array}{lllc}
\hline f \text {-divergence } & \boldsymbol{f}(\boldsymbol{u}) & \boldsymbol{f}^{\prime}(\boldsymbol{u}) \\
\hline \alpha \text {-divergence }(\alpha \in(0,1)) & \left(u^{1-\alpha}-(1-\alpha) u-\alpha\right) /(\alpha(\alpha-1)) & \left(1-u^{-\alpha}\right) / \alpha\\
\text { Reverse KL }(\alpha=0) & u \log u & \log u+1\\
\text { Forward KL }(\alpha=1) & -\log u & -1 / u\\
\text { JS-divergence } & u \log u-(u+1) \log ((u+1) / 2) & \log (2 u /(1+u))\\
\hline
\end{array}
$$

В файле `utils.py` содержатся функции потерь, которые получаются из других функций дивергенции. А именно:
- RKL_divergence
- KL_divergence
- alpha_divergence
- JS_divergence
То, как функции встроены, содержится в `pipeline.py`.
График, который был получен в результате, отличается от представленного в статье:

![newplot](https://github.com/Lerostre/test-task-alignment/assets/90250718/78d0634c-2982-417a-b4ab-4a9e5b165a12)


Возникнуть такая ситуация могла по следующим причинам:
- Были использованы не промпты из датасета IMDB, как в статье, а авторские.
- Diversity уже изначально было неплохим в силу использования других промптов.
- Разный подход к обучению моделей. В статье модель обучалась до полной сходимости и на большем количестве примеров, здесь же количество примеров было ограничено имеющимися ресурсами, кроме того, число эпох было определено экспериментально, поскольку на валидации было заметно переобучение.
Тем не менее, общая картина получилась очень похожей на ту, что показана в статье. И важно, что информация на графике интерпретируема. Так, KL создает разнообразные тексты, в том числе, негативные, так что reward получается меньше, а entropy -- довольно высокой. RKL же ведет себя иначе, поскольку не является zero-forced:



### Level 3.

Способы улучшить представленные методы: 
Основной идеей здесь была попытка протестировать другие функции дивергенции и посмотреть, как они влияют на поведение модели. Часть из них -- Total Variation и Chi-squared -- уже была представлена в статье, но не рассмотрена подробно.

$$
\begin{array}{lllc}
\hline f \text {-divergence } & \boldsymbol{f}(\boldsymbol{u}) & \boldsymbol{f}^{\prime}(\boldsymbol{u}) \\
\hline
\text { Pearson } \chi^2 & (u-1)^2 & 2(u-1)\\
\text { Neyman } \chi^2 & (1-u)^2 / u & 1 - \frac{1}{u^2}\\
\text { Hellinger } & (\sqrt{u}-1)^2 & 1 - \frac{1}{\sqrt{u}}\\
\text { Jeffrey } & (u-1)\log{u} &  \log u + 1 - \frac{1}{u}\\
\text { GAN } & u\log{u} - (u+1)\log(u+1) & \log u - \log(u+1)\\
\text { Total Variation } & \frac{1}{2}|u-1| & u>1 ? \frac{1}{2}:-\frac{1}{2}\\
\hline
\end{array}
$$

Очевидно, что у каждой функции будут свои особенности............ 
